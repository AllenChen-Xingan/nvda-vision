# User Stories: NVDA Vision Screen Reader

**Document Version**: v1.0.0
**Created Date**: 2025-12-24
**Depends on**: `.42cog/real/real.md`, `.42cog/cog/cog.md`, `spec/pm/pr.spec.md`
**Generated by**: pm-user-story skill (Three Minimal Stories Framework)

---

## Executive Summary

This document decomposes the NVDA Vision Screen Reader product requirements into executable user stories using the **Three Minimal Stories Framework** (Light, Dark, Grey). Each story follows INVEST principles and includes detailed acceptance criteria, user-perceivable changes, and constraints from reality.

**Project Goal**: Enable visually impaired users to independently operate custom UI applications (Feishu, DingTalk, etc.) through AI-powered visual recognition and keyboard navigation.

**Story Type Distribution (MVP Stage)**:
- Light Stories: 70% (user growth, problem solving)
- Dark Stories: 20% (error handling, exception cases)
- Grey Stories: 10% (repetitive operations, daily use)

---

## Complex Story Overview

| Complex Story | MS Count | Priority | Story Type Distribution | User Journey Phase |
|--------------|----------|----------|------------------------|-------------------|
| CS-01: First Vision Recognition | 5 | P0 | Light 3, Dark 1, Grey 1 | Onboarding → First Success |
| CS-02: Element Navigation & Activation | 6 | P0 | Light 3, Dark 2, Grey 1 | Core Usage |
| CS-03: Recognition Uncertainty Handling | 4 | P1 | Light 1, Dark 2, Grey 1 | Trust Building |
| CS-04: Configuration & Optimization | 4 | P2 | Light 2, Dark 1, Grey 1 | Power User |

**Total**: 19 Minimal Stories (Light: 9, Dark: 6, Grey: 4)

---

# Complex Stories & Minimal Stories

## CS-01: First Vision Recognition

**Description**: User's first complete journey from triggering AI recognition to hearing results, experiencing "from invisible to perceivable."

**User Type**: Newly onboarded blind user (NVDA proficient, first-time AI vision tool user)

**Core Value**: Enable users to perceive UI elements that were previously completely invisible to them

**Story Type Distribution**:
- Light Stories: 3 (first trigger, first hearing results, cache speedup)
- Dark Stories: 1 (recognition failure handling)
- Grey Stories: 1 (repeated use of trigger shortcut)

**User Journey**:
[User opens Feishu] → [Presses NVDA+Shift+V] → [Hears "Recognizing screen..."] → [Hears element list] → [Feels affordance restored]

---

### MS-L-01: Trigger First AI Recognition

**Complex Story**: CS-01
**Story Type**: Light (Bad to Good)
**Priority**: P0
**Story Points**: 3

#### User Story

As a **blind Feishu user**,
I want to **press NVDA+Shift+V to trigger AI screen recognition**,
So that **I can perceive UI elements that were previously invisible to me**.

#### Story Evaluation (Light Story)

**Starting State**: User opens Feishu, NVDA reads very limited information, unable to locate buttons like "Send Message"

**Turning Point**: User discovers and presses the NVDA+Shift+V shortcut, hears "Recognizing screen, please wait..."

**End State**: User perceives that the system is helping them "see" the screen, experiencing being helped

**Emotional Experience**: From confusion/helplessness → discovery/anticipation → being understood/valued

#### Acceptance Criteria

**AC1: Shortcut Registration Success**
- Given: User has installed NVDA Vision plugin
- When: User presses NVDA+Shift+V
- Then: System captures keyboard event and triggers recognition process

**AC2: Immediate Audio Feedback**
- Given: Recognition has been triggered
- When: Within 100ms
- Then: NVDA reads "Recognizing screen, please wait..."

**AC3: Independent Thread Execution**
- Given: Recognition is in progress
- When: User continues to operate NVDA (e.g., presses NVDA+T to read window title)
- Then: NVDA functions normally, unaffected

**AC4: Screenshot Success**
- Given: Recognition triggered
- When: System captures current active window
- Then: Generated PNG format screenshot, dimensions ≤ 1920px

**AC5: First-time Guidance**
- Given: User triggers recognition for the first time
- When: Before recognition starts
- Then: System reads "First use? This will use a local AI model to recognize UI elements, with data processed locally"

#### User-Perceivable Changes

**Before Operation**:
- User hears only limited information NVDA can read
- User feels confused, unsure how to locate buttons
- User may need to ask others for help

**During Operation**:
- User presses key combination (muscle memory: NVDA+Shift+V)
- User hears immediate voice feedback "Recognizing..."
- User waits for results (3-15 seconds)

**After Operation**:
- User perceives system is "helping them see"
- User feels anticipation and hope
- User is ready to hear recognition results

#### Constraints (from real.md)

- MUST execute in independent thread, cannot block NVDA main program
- MUST provide immediate audio feedback (< 100ms)
- MUST comply with WCAG 2.1 AA standard, keyboard fully accessible
- MUST handle screenshot failure exceptions gracefully
- Screenshot data MUST be processed locally first, NOT uploaded to cloud automatically

#### Dependencies

**Requires**:
- NVDA core program is running normally
- Plugin installation is complete
- User understands basic NVDA shortcut usage

**Enables**:
- MS-L-02: Hear AI Recognition Results
- MS-D-01: Handle Recognition Failures
- MS-G-01: Repeated Trigger Recognition

---

### MS-L-02: Hear AI Recognition Results

**Complex Story**: CS-01
**Story Type**: Light (Bad to Good)
**Priority**: P0
**Story Points**: 5

#### User Story

As a **blind Feishu user**,
I want to **hear a clear, understandable list of UI elements after AI recognition completes**,
So that **I can know what buttons and input fields are available on the current screen**.

#### Story Evaluation (Light Story)

**Starting State**: User is waiting for recognition results, in a state of anticipation

**Turning Point**: System completes recognition (3-15 seconds), starts reading results

**End State**: User hears "Found 3 buttons: Send Message, Cancel, Attachment, 1 input field: Message content", understanding the screen layout

**Emotional Experience**: From anticipation/anxiety → clear understanding → sense of control restored

#### Acceptance Criteria

**AC1: Recognition Completes Within Timeframe**
- Given: Recognition triggered successfully
- When: Within 15 seconds
- Then: System returns recognition results OR automatically downgrades

**AC2: Results in Structured Audio Format**
- Given: Recognition returns 3 buttons, 1 input field
- When: NVDA reads results
- Then: Format: "Found 3 buttons: [button1], [button2], [button3], 1 input field: [field name]"

**AC3: Element Type Categorization**
- Given: Recognition results include multiple element types
- When: Generating audio text
- Then: Group by type (buttons first, then input fields, then links)

**AC4: Interactive Elements Only**
- Given: Recognition returns 20 elements (including non-interactive text)
- When: Filtering results
- Then: Only read interactive elements (type in [button, textbox, link, dropdown, checkbox])

**AC5: Confidence Transparency**
- Given: An element's confidence score < 0.7
- When: Reading this element
- Then: Add "Uncertain:" prefix, e.g., "Uncertain: Button: Might be Submit, 65% confidence"

**AC6: Empty Result Handling**
- Given: Recognition returns 0 interactive elements
- When: Generating audio
- Then: Read "No interactive elements recognized, you can try: 1. Re-recognize (NVDA+Shift+R) 2. Check if window is active"

#### User-Perceivable Changes

**Before Operation**:
- User is in waiting state, hearing no new information
- User may feel anxious (is system working?)

**During Operation**:
- User hears model inference progress (if > 5 seconds: "Recognizing, 8 seconds elapsed...")
- User can press Esc to cancel

**After Operation**:
- User hears complete element list
- User establishes mental model of screen (knows what's available)
- User can decide next action (navigate, activate, or re-recognize)

#### Constraints (from real.md)

- MUST display progress prompt if inference time > 5 seconds
- MUST auto-downgrade if > 15 seconds (GPU → CPU → Cloud API)
- All recognition results MUST include confidence score
- Low confidence results (< 0.7) MUST be marked "Uncertain"
- Recognition failure MUST NOT crash NVDA main program
- Generated audio text MUST comply with NVDA TTS specifications

#### Dependencies

**Requires**:
- MS-L-01: Trigger First AI Recognition (completed)
- Vision model loaded successfully
- Screenshot data ready

**Enables**:
- MS-L-03: Navigate to Specific Element
- MS-L-04: Activate Target Element
- MS-D-02: Handle Low Confidence Results

---

### MS-L-03: Experience Cache Acceleration

**Complex Story**: CS-01
**Story Type**: Light (Bad to Good)
**Priority**: P1
**Story Points**: 2

#### User Story

As a **blind user who frequently switches windows**,
I want **the system to instantly return results when I re-recognize the same screen**,
So that **I don't have to wait repeatedly for AI inference**.

#### Story Evaluation (Light Story)

**Starting State**: User recognizes the same Feishu window again, expecting to wait another 5-10 seconds

**Turning Point**: System detects cache hit (same screenshot hash), instantly returns results

**End State**: User hears results < 100ms, experiencing "wow, so fast!"

**Emotional Experience**: From resigned to waiting → surprised by speed → appreciating system intelligence

#### Acceptance Criteria

**AC1: Screenshot Hash Matching**
- Given: User re-recognizes the same window
- When: System calculates screenshot SHA-256 hash
- Then: Match found in cache (within 5 minutes)

**AC2: Instant Return**
- Given: Cache hit
- When: User triggers recognition
- Then: Results returned < 100ms, audio feedback "Found [X] buttons..."

**AC3: Cache Transparency**
- Given: Results from cache
- When: User presses NVDA+Shift+D to view details
- Then: Read "This result from cache, recognized [X] seconds ago"

**AC4: Cache Expiration**
- Given: Cache entry created 5 minutes ago
- When: Background task checks expiration
- Then: Auto-delete expired entry

**AC5: Cache Size Limit**
- Given: Cache has 100 entries
- When: Adding new recognition result
- Then: Delete oldest entry (FIFO strategy)

#### User-Perceivable Changes

**Before Operation**:
- User expects normal 5-10 second wait
- User may feel impatient or inefficient

**During Operation**:
- User presses NVDA+Shift+V
- Almost no waiting

**After Operation**:
- User immediately hears results
- User realizes system is "remembering" previous recognitions
- User feels efficiency improved

#### Constraints (from real.md)

- Cache MUST use SHA-256 hash to ensure accuracy
- Cache TTL default 5 minutes (configurable)
- Cache size max 100 entries (prevent memory overflow)
- Cache data MUST NOT include sensitive information (only metadata)
- Cache mechanism MUST NOT affect privacy protection

#### Dependencies

**Requires**:
- MS-L-01: Trigger First AI Recognition
- MS-L-02: Hear AI Recognition Results

**Enables**:
- MS-G-01: Repeated Trigger Recognition (optimizes experience)

---

### MS-D-01: Handle Recognition Failures

**Complex Story**: CS-01
**Story Type**: Dark (Good to Bad to Recovery)
**Priority**: P0
**Story Points**: 3

#### User Story

As a **blind user**,
I want **clear error messages and recovery suggestions when AI recognition fails**,
So that **I know what went wrong and what to try next**.

#### Story Evaluation (Dark Story)

**Starting State**: User smoothly uses recognition feature, expecting normal results

**Turning Point**: System encounters error (model loading failure, GPU OOM, API call failure)

**End State**: System reads "Recognition failed: GPU out of memory, auto-switching to CPU model, please try again", user understands issue and knows how to recover

**Emotional Experience**: From smooth → blocked/confused → helped/safe

#### Acceptance Criteria

**AC1: Exception Capture**
- Given: Model inference throws exception
- When: Exception occurs
- Then: System catches all exceptions (try-except), logs details, DOES NOT crash NVDA

**AC2: User-Friendly Error Messages**
- Given: GPU out of memory error
- When: Generating error message
- Then: Read "Recognition failed: Insufficient GPU memory, auto-switching to CPU model, please try again"

**AC3: Auto-Downgrade Strategy**
- Given: Primary model (UI-TARS GPU) fails
- When: Triggering downgrade
- Then: Auto-try secondary model (MiniCPM CPU), if still fails → try cloud API

**AC4: Downgrade Notification**
- Given: System switches from GPU to CPU model
- When: Before re-recognizing
- Then: Read "Primary model unavailable, using backup model"

**AC5: Persistent Failures**
- Given: All models (GPU, CPU, Cloud) fail
- When: Generating final error message
- Then: Read "Recognition failed, possible reasons: 1. Screenshot failed 2. Network disconnected 3. Models not installed. View detailed logs? Press L"

**AC6: Recovery Suggestions**
- Given: Recognition failed
- When: Reading error message
- Then: Provide actionable suggestions (retry, check network, view logs)

#### User-Perceivable Changes

**Before Operation**:
- User smoothly triggers recognition, expecting normal results

**During Operation**:
- User waits but system encounters error
- User hears error message instead of results

**After Operation**:
- User understands error cause
- User receives recovery suggestions
- User can choose to: retry, switch model, or view logs

#### Constraints (from real.md)

- Plugin crashes or model inference failures MUST NOT affect NVDA main program
- MUST use independent threads and comprehensive exception handling
- Error logs MUST NOT include sensitive info (screenshots, API keys)
- User MUST be able to continue using NVDA even after plugin fails
- Timeout mechanism MUST prevent threads from hanging indefinitely

#### Dependencies

**Requires**:
- MS-L-01: Trigger First AI Recognition

**Enables**:
- MS-L-06: Manual Re-recognize (provides recovery path)
- MS-L-08: Configure Model Priority (advanced users adjust strategy)

---

### MS-G-01: Repeated Trigger Recognition

**Complex Story**: CS-01
**Story Type**: Grey (Cyclical)
**Priority**: P1
**Story Points**: 1

#### User Story

As a **daily NVDA user**,
I want **the shortcut NVDA+Shift+V to become my muscle memory**,
So that **I can instantly trigger recognition when needed without thinking**.

#### Story Evaluation (Grey Story)

**Cycle Pattern**: User triggers recognition 10+ times per day (each time switching windows, opening apps, UI changes)

**Optimization Point**: Shortcut design is easily memorable, ergonomic, doesn't conflict

**Habit Formation**: From "need to think which key" → finger auto-presses keys → "becomes second nature"

**Emotional Experience**: From effortful (conscious control) → easy (skilled operation) → automated (subconscious habit)

#### Acceptance Criteria

**AC1: Shortcut Consistency**
- Given: User uses plugin for 1 month
- When: Shortcut binding checked
- Then: NVDA+Shift+V remains consistent, unchanged

**AC2: No Conflict**
- Given: User has other NVDA plugins installed
- When: System checks shortcut conflicts
- Then: NVDA+Shift+V doesn't conflict with NVDA core or other plugins

**AC3: Shortcut Hint**
- Given: User installs plugin
- When: First launches NVDA
- Then: System reads "New shortcut: NVDA+Shift+V triggers screen recognition"

**AC4: Ergonomic Design**
- Given: Shortcut is NVDA+Shift+V
- When: User presses on standard keyboard
- Then: Left hand can complete operation (CapsLock+Shift+V or Insert+Shift+V)

**AC5: High-Frequency Usage Optimization**
- Given: User triggers recognition 50+ times per day
- When: Cache mechanism enabled
- Then: ≥ 30% requests cache hit, significantly reducing wait time

#### User-Perceivable Changes

**Before Operation** (First Week):
- User needs to consciously think "which key triggers recognition?"
- User may press wrong keys, need to try again

**During Operation** (Weeks 2-4):
- User starts forming muscle memory
- Fingers automatically find key positions

**After Operation** (After 1 Month):
- User can "trigger recognition without thinking"
- Shortcut becomes "extension of body"

#### Constraints (from real.md)

- MUST comply with WCAG 2.1 AA standard, keyboard fully accessible
- Shortcut design MUST consider ergonomics, frequently used shortcuts easily pressed
- MUST auto-detect shortcut conflicts, warn user

#### Dependencies

**Requires**:
- MS-L-01: Trigger First AI Recognition
- MS-L-03: Experience Cache Acceleration

**Enables**:
- All subsequent navigation and activation operations (become daily workflow)

---

## CS-02: Element Navigation & Activation

**Description**: After user gets recognition results, they navigate through element list and activate target elements (click, input)

**User Type**: Blind user familiar with basic recognition process

**Core Value**: Complete full interaction flow from "perceiving elements" to "executing operations"

**Story Type Distribution**:
- Light Stories: 3 (first navigation, first activation, successful operation)
- Dark Stories: 2 (low-confidence element activation, click failure)
- Grey Stories: 1 (daily navigation operations)

**User Journey**:
[Hear element list] → [Press N/P to navigate] → [Locate target element] → [Press Enter to activate] → [App responds]

---

### MS-L-04: Navigate to Specific Element

**Complex Story**: CS-02
**Story Type**: Light (Bad to Good)
**Priority**: P0
**Story Points**: 3

#### User Story

As a **blind user**,
I want to **use NVDA+Shift+N/P to navigate forward/backward through recognized elements**,
So that **I can locate the target button or input field I want to operate**.

#### Story Evaluation (Light Story)

**Starting State**: User hears "Found 5 buttons, 2 input fields" but doesn't know how to select specific element

**Turning Point**: User presses NVDA+Shift+N, hears "Button: Send Message"

**End State**: User continues pressing N, sequentially hears all elements, finds target element

**Emotional Experience**: From "know what exists" → "can navigate" → "control focus"

#### Acceptance Criteria

**AC1: Navigation Shortcut**
- Given: Recognition results include 5 elements
- When: User presses NVDA+Shift+N
- Then: System moves focus to next element, reads type and text

**AC2: Reverse Navigation**
- Given: Current focus on 3rd element
- When: User presses NVDA+Shift+P
- Then: System moves focus to 2nd element, reads info

**AC3: Circular Navigation**
- Given: Current focus on last element
- When: User presses NVDA+Shift+N
- Then: System returns to first element, reads "Reached list end, returning to start: [element1]"

**AC4: Element Info Reading**
- Given: Focus moves to new element
- When: System generates audio
- Then: Format: "[Type]: [Text]", e.g., "Button: Send Message"

**AC5: Confidence Marking**
- Given: Current element confidence < 0.7
- When: Reading element info
- Then: Add prefix: "Uncertain: Button: Might be Submit, 65% confidence"

**AC6: Navigation State Feedback**
- Given: User navigates
- When: After each navigation
- Then: Read "[current position]/[total count], [element info]", e.g., "2/5, Input field: Message content"

#### User-Perceivable Changes

**Before Operation**:
- User knows element list but can't select specific element
- User feels "can see but can't touch"

**During Operation**:
- User presses N key continuously
- Hears one element at a time
- Can press P to go back

**After Operation**:
- User locates target element
- User feels "I can control focus now"
- User is ready to activate element

#### Constraints (from real.md)

- MUST comply with WCAG 2.1 AA, keyboard fully accessible
- Navigation MUST only include interactive elements (button, link, textbox)
- Low-confidence elements (< 0.7) MUST have "Uncertain" marking
- Navigation operations MUST NOT block NVDA main thread

#### Dependencies

**Requires**:
- MS-L-02: Hear AI Recognition Results

**Enables**:
- MS-L-05: Activate Target Element
- MS-L-07: View Element Details

---

### MS-L-05: Activate Target Element

**Complex Story**: CS-02
**Story Type**: Light (Bad to Good)
**Priority**: P0
**Story Points**: 5

#### User Story

As a **blind user**,
I want to **press Enter to activate (click) the currently focused element**,
So that **I can actually execute operations (send message, submit form, etc.)**.

#### Story Evaluation (Light Story)

**Starting State**: User navigates to "Button: Send Message" but hasn't completed operation

**Turning Point**: User presses Enter key

**End State**: System simulates mouse click button center coordinates, Feishu sends message, user hears Feishu's response "Message sent"

**Emotional Experience**: From "can navigate" → "can actually operate" → "independent use achieved"

#### Acceptance Criteria

**AC1: Activation Shortcut**
- Given: Current focus on "Button: Send Message"
- When: User presses Enter
- Then: System simulates mouse click button center coordinates

**AC2: Coordinate Calculation Accuracy**
- Given: Element bbox is [x1=520, y1=340, x2=620, y2=380]
- When: Calculating click coordinates
- Then: Click center point (570, 360)

**AC3: Pre-click Cursor Move**
- Given: System will simulate click
- When: Before clicking
- Then: First move cursor to target coordinates, then click (prevent misclicks)

**AC4: Activation Feedback**
- Given: User presses Enter
- When: Before executing click
- Then: System reads "Activating: [element type]: [element text]"

**AC5: Activation Delay**
- Given: System executes click
- When: After click completes
- Then: Wait 200ms, then read "Activated, please wait for app response"

**AC6: Low-Confidence Warning**
- Given: Element confidence < 0.7
- When: User presses Enter
- Then: First read "This element is uncertain (65% confidence), confirm activation? Press Y to confirm, N to cancel"

#### User-Perceivable Changes

**Before Operation**:
- User has located target element
- User is ready to execute operation

**During Operation**:
- User presses Enter
- Hears "Activating: Button: Send Message"
- Brief wait (< 500ms)

**After Operation**:
- User hears "Activated, please wait for app response"
- User waits for app feedback (e.g., Feishu prompts "Message sent")
- User feels "I did it myself!"

#### Constraints (from real.md)

- MUST calculate precise click coordinates (element center)
- MUST move cursor first, then click (prevent misclicks)
- Low-confidence elements (< 0.7) MUST require secondary confirmation
- Click operations MUST NOT crash NVDA main program
- MUST handle click failures (e.g., element no longer exists)

#### Dependencies

**Requires**:
- MS-L-04: Navigate to Specific Element
- System permission to simulate mouse operations

**Enables**:
- All actual app operations (send messages, fill forms, open menus, etc.)
- MS-L-09: Complete First Workflow (full flow from recognition to operation)

---

### MS-L-06: Manual Re-recognize

**Complex Story**: CS-02
**Story Type**: Light (Bad to Good)
**Priority**: P1
**Story Points**: 2

#### User Story

As a **blind user**,
I want to **press NVDA+Shift+R to manually re-trigger recognition**,
So that **I can get latest UI state when page content changes (e.g., popup, new message)**.

#### Story Evaluation (Light Story)

**Starting State**: User activates a button, page pops up new dialog, but recognition result list hasn't updated

**Turning Point**: User presses NVDA+Shift+R, triggers new recognition

**End State**: User hears new element list, including popup elements

**Emotional Experience**: From "UI changed, I don't know new content" → "I can actively refresh" → "regain situation awareness"

#### Acceptance Criteria

**AC1: Re-recognize Shortcut**
- Given: User has triggered recognition once
- When: User presses NVDA+Shift+R
- Then: System ignores cache, forces new screenshot and recognition

**AC2: Ignore Cache**
- Given: Current screenshot hash exists in cache
- When: User triggers re-recognize
- Then: System recalculates, doesn't use cached results

**AC3: Re-recognize Notification**
- Given: User presses R
- When: Before recognition starts
- Then: System reads "Re-recognizing screen, ignoring cache..."

**AC4: Results Comparison Hint**
- Given: Re-recognition completes
- When: Generating audio
- Then: Optionally read "Compared to last time: 2 new elements, 1 element disappeared"

#### User-Perceivable Changes

**Before Operation**:
- User senses UI has changed (e.g., heard popup sound)
- User's recognition result list is outdated

**During Operation**:
- User presses NVDA+Shift+R
- Hears "Re-recognizing screen..."
- Waits for new results (3-15 seconds)

**After Operation**:
- User hears updated element list
- User regains understanding of current UI state

#### Constraints (from real.md)

- Re-recognize MUST ignore cache, force new inference
- MUST handle re-recognize during recognition (cancel previous one)
- Re-recognize operations MUST NOT cause memory leaks

#### Dependencies

**Requires**:
- MS-L-02: Hear AI Recognition Results

**Enables**:
- MS-D-02: Handle Low Confidence Results (try backup model)

---

### MS-L-07: View Element Details

**Complex Story**: CS-02
**Story Type**: Light (Bad to Good)
**Priority**: P2
**Story Points**: 2

#### User Story

As a **advanced blind user or developer**,
I want to **press NVDA+Shift+D to view detailed information of current element (coordinates, confidence, model)**,
So that **I can debug recognition issues or learn system behavior**.

#### Story Evaluation (Light Story)

**Starting State**: User wants to understand why an element recognition is uncertain or wants to know technical details

**Turning Point**: User presses NVDA+Shift+D

**End State**: User hears "Type: button, Text: Send Message, Coordinates: 520,340 to 620,380, Confidence: 85%, Model: UI-TARS 7B GPU"

**Emotional Experience**: From "curiosity/doubt" → "get detailed info" → "understand system better"

#### Acceptance Criteria

**AC1: Details Shortcut**
- Given: Current focus on an element
- When: User presses NVDA+Shift+D
- Then: System reads element detailed info

**AC2: Information Completeness**
- Given: Requesting details
- When: Generating audio
- Then: Include: type, text, coordinates, confidence, model name, recognition time

**AC3: User-Friendly Format**
- Given: Reading details
- When: Generating audio
- Then: Natural language, e.g., "Button, text 'Send Message', screen coordinates upper-left 520,340 to lower-right 620,380, 85% confidence, recognized 3 seconds ago using UI-TARS 7B GPU model"

#### User-Perceivable Changes

**Before Operation**:
- User only knows basic element info
- User may be curious about technical details

**During Operation**:
- User presses NVDA+Shift+D
- Immediately hears detailed info

**After Operation**:
- User gets complete technical info
- User can report bugs or optimize usage strategy based on this info

#### Constraints (from real.md)

- Details MUST NOT include sensitive info (e.g., screenshot content)
- Coordinates readable in natural language (not just numbers)

#### Dependencies

**Requires**:
- MS-L-04: Navigate to Specific Element

**Enables**:
- Advanced users debug and provide feedback
- Developers verify model performance

---

### MS-D-02: Handle Low Confidence Results

**Complex Story**: CS-02
**Story Type**: Dark (Good to Bad to Recovery)
**Priority**: P0
**Story Points**: 3

#### User Story

As a **blind user**,
I want **clear warnings when system recognizes uncertain elements and can choose whether to activate**,
So that **I can avoid misoperations (e.g., clicking wrong button)**.

#### Story Evaluation (Dark Story)

**Starting State**: User smoothly navigates, expects accurate results

**Turning Point**: User hears "Uncertain: Button: Might be Submit, 65% confidence"

**End State**: User presses Enter, system asks "This element uncertain (65% confidence), confirm activation? Y=confirm, N=cancel, R=re-recognize", user makes informed choice

**Emotional Experience**: From confident → alerted/cautious → given control/safe

#### Acceptance Criteria

**AC1: Confidence Threshold Marking**
- Given: Element confidence = 0.65
- When: Reading element info
- Then: Add "Uncertain:" prefix and report confidence percentage

**AC2: Activation Confirmation**
- Given: User activates low-confidence element (< 0.7)
- When: Before executing click
- Then: System reads "This element uncertain (65% confidence), confirm activation? Y=confirm, N=cancel, R=re-recognize with backup model"

**AC3: Cancel Mechanism**
- Given: Confirmation prompt appears
- When: User presses N or Esc
- Then: Cancel activation, keep current focus

**AC4: Re-recognize Mechanism**
- Given: Confirmation prompt appears
- When: User presses R
- Then: Trigger re-recognize with backup model (e.g., GPU → Cloud API)

**AC5: Confirm Mechanism**
- Given: Confirmation prompt appears
- When: User presses Y or Enter
- Then: Continue execution, click element coordinates

**AC6: Confidence Threshold Configurable**
- Given: User wants to adjust warning threshold
- When: In config file
- Then: Can set `confidence_warning_threshold: 0.6 ~ 0.8` (default 0.7)

#### User-Perceivable Changes

**Before Operation**:
- User expects normal element

**During Operation**:
- User hears "Uncertain" warning
- User feels alerted but not panicked
- User given choice (confirm/cancel/re-recognize)

**After Operation**:
- If confirmed: operation executed, user responsible for choice
- If canceled: stays safe, user can re-navigate
- If re-recognized: tries backup model, potentially higher confidence

#### Constraints (from real.md)

- All UI element recognition results MUST include confidence score
- Low-confidence results (< threshold) MUST be marked "Uncertain"
- MUST provide manual re-recognize option
- Confidence threshold MUST be user-configurable

#### Dependencies

**Requires**:
- MS-L-04: Navigate to Specific Element
- MS-L-05: Activate Target Element

**Enables**:
- MS-L-06: Manual Re-recognize (recovery path)
- MS-L-10: Configure Confidence Threshold (advanced customization)

---

### MS-D-03: Handle Click Failures

**Complex Story**: CS-02
**Story Type**: Dark (Good to Bad to Recovery)
**Priority**: P1
**Story Points**: 2

#### User Story

As a **blind user**,
I want **clear error messages and recovery suggestions when click fails (e.g., element disappeared, app unresponsive)**,
So that **I know what went wrong and what to try next**.

#### Story Evaluation (Dark Story)

**Starting State**: User smoothly clicks elements, expects app response

**Turning Point**: System executes click but app doesn't respond, or element has disappeared

**End State**: System reads "Click may have failed, app unresponsive. Suggestions: 1. Re-recognize (R) 2. Check if app crashed", user understands situation

**Emotional Experience**: From confident → confused → helped/reassured

#### Acceptance Criteria

**AC1: App Response Detection**
- Given: Click executed
- When: Waiting 2 seconds
- Then: System detects if app window responds (not crashed/frozen)

**AC2: Element Existence Check**
- Given: Ready to click element
- When: Before clicking
- Then: Optionally re-recognize small area to verify element still exists

**AC3: Failure Notification**
- Given: App unresponsive or click failed
- When: Generating error message
- Then: Read "Click may have failed, possible reasons: 1. Element disappeared 2. App unresponsive 3. Coordinates incorrect"

**AC4: Recovery Suggestions**
- Given: Click failed
- When: After error message
- Then: Read "Suggestions: Press R to re-recognize, Press Esc to cancel operation"

#### User-Perceivable Changes

**Before Operation**:
- User expects smooth operation

**During Operation**:
- User clicks but no app response
- User feels confused "did it work?"

**After Operation**:
- User hears failure notification
- User gets actionable suggestions
- User can choose retry or cancel

#### Constraints (from real.md)

- Click failures MUST NOT crash NVDA
- MUST detect app freeze/crash (within reasonable time)
- Error messages MUST be user-friendly, not technical jargon

#### Dependencies

**Requires**:
- MS-L-05: Activate Target Element

**Enables**:
- MS-L-06: Manual Re-recognize (recovery path)

---

### MS-G-02: Daily Element Navigation

**Complex Story**: CS-02
**Story Type**: Grey (Cyclical)
**Priority**: P1
**Story Points**: 1

#### User Story

As a **daily user**,
I want **N/P keys to become my navigation muscle memory**,
So that **I can quickly locate target elements without thinking**.

#### Story Evaluation (Grey Story)

**Cycle Pattern**: User navigates elements 30+ times per day (every operation requires navigation)

**Optimization Point**: Navigation shortcuts easily memorable, ergonomic, fast response

**Habit Formation**: From "need to think N=next P=previous" → finger auto-presses keys → "navigation becomes breathing"

**Emotional Experience**: From effortful (conscious thinking) → easy (skilled operation) → automated (subconscious habit)

#### Acceptance Criteria

**AC1: Quick Response**
- Given: User presses N
- When: System processes navigation
- Then: Audio feedback < 100ms (nearly instant)

**AC2: Continuous Navigation Optimization**
- Given: User presses N 5 times consecutively
- When: System generates audio
- Then: Simplify format, only read "[element type]: [text]", omit position info

**AC3: Boundary Feedback**
- Given: User navigates to last element, presses N again
- When: Returning to first element
- Then: Audio hint "Reached list end, returning to start"

**AC4: Jump Navigation (optional)**
- Given: Element list has 20 items
- When: User presses NVDA+Shift+J
- Then: Read "Jump to which element? Enter number 1-20", user inputs number to quick jump

#### User-Perceivable Changes

**Before Operation** (First Week):
- User needs to think "N=next, P=previous"
- Navigation may be slow, error-prone

**During Operation** (Weeks 2-4):
- User starts forming muscle memory
- Navigation speed increases

**After Operation** (After 1 Month):
- User can "navigate without thinking"
- N/P keys become "extension of body"

#### Constraints (from real.md)

- Navigation response MUST be fast (< 100ms)
- MUST comply with WCAG 2.1 AA, keyboard fully accessible
- Long element lists (> 10) SHOULD provide jump navigation

#### Dependencies

**Requires**:
- MS-L-04: Navigate to Specific Element

**Enables**:
- All daily operation workflows (become efficient daily tools)

---

## CS-03: Recognition Uncertainty Handling

**Description**: When users encounter low-confidence or failed recognitions, system provides transparent information and multiple recovery paths

**User Type**: Blind users who have used recognition feature several times, starting to encounter edge cases

**Core Value**: Build trust through transparency, cultivate users' "critical AI literacy"

**Story Type Distribution**:
- Light Stories: 1 (understand AI limitations)
- Dark Stories: 2 (handle failures, cloud authorization)
- Grey Stories: 1 (learn to judge confidence)

**User Journey**:
[Encounter low confidence] → [See transparency] → [Choose recovery path] → [Re-recognize or authorize cloud] → [Get better results]

---

### MS-L-08: Understand AI Limitations

**Complex Story**: CS-03
**Story Type**: Light (Bad to Good)
**Priority**: P1
**Story Points**: 2

#### User Story

As a **blind user**,
I want to **understand AI recognition isn't 100% accurate through gradual experience**,
So that **I can build realistic expectations and learn when to trust results**.

#### Story Evaluation (Light Story)

**Starting State**: User initially thinks AI is omnipotent, expecting perfect results every time

**Turning Point**: User encounters several "Uncertain" results, and MS-L-07 views details, realizes confidence varies

**End State**: User understands "AI is a tool that helps but isn't perfect", establishes realistic expectations

**Emotional Experience**: From blind trust → confusion → rational understanding → mature usage

#### Acceptance Criteria

**AC1: First Low-Confidence Guidance**
- Given: User encounters "Uncertain" result for first time
- When: After reading element info
- Then: System additionally reads "Note: AI isn't 100% accurate, low-confidence results marked as 'Uncertain', you can press R to re-recognize"

**AC2: Confidence Distribution Transparency**
- Given: Recognition results include multiple elements
- When: User presses NVDA+Shift+S (statistics)
- Then: System reads "This recognition: 3 high-confidence (>80%), 2 medium-confidence (70-80%), 1 low-confidence (<70%)"

**AC3: Model Performance Description**
- Given: User views element details (MS-L-07)
- When: Reading model info
- Then: Include model accuracy description, e.g., "UI-TARS 7B, average accuracy 83%"

#### User-Perceivable Changes

**Before Operation**:
- User has unrealistic expectations of AI
- User may blame themselves when encountering errors

**During Operation**:
- User repeatedly experiences varying confidences
- User gradually understands AI capabilities and limitations

**After Operation**:
- User establishes realistic expectations
- User learns when to trust results, when to re-verify
- User feels empowered (understands tools)

#### Constraints (from real.md)

- Transparency is key to building user trust
- MUST provide clear confidence marking, not hide AI limitations
- Educational guidance SHOULD be gradual, not overwhelming

#### Dependencies

**Requires**:
- MS-L-02: Hear AI Recognition Results
- MS-L-07: View Element Details

**Enables**:
- MS-D-02: Handle Low Confidence Results (users know why confirmations are needed)
- MS-L-10: Configure Confidence Threshold (users understand parameter meaning)

---

### MS-D-04: Handle Recognition Timeout

**Complex Story**: CS-03
**Story Type**: Dark (Good to Bad to Recovery)
**Priority**: P0
**Story Points**: 3

#### User Story

As a **blind user**,
I want **auto-downgrade or cancel options when recognition takes too long (>15 seconds)**,
So that **I'm not stuck in endless waiting, can choose to continue or cancel**.

#### Story Evaluation (Dark Story)

**Starting State**: User triggers recognition, expecting normal 3-5 second return

**Turning Point**: Waits 10 seconds, hears "Recognizing, 10 seconds elapsed..." still no results

**End State**: At 15 seconds, system reads "Recognition taking too long, auto-switching to backup model", or prompts "Continue? Enter=yes, Esc=cancel"

**Emotional Experience**: From expecting → impatient/anxious → given control/relieved

#### Acceptance Criteria

**AC1: 5-Second Progress Prompt**
- Given: Recognition exceeds 5 seconds
- When: Every 3 seconds thereafter
- Then: System reads "Recognizing, [X] seconds elapsed..."

**AC2: 15-Second Auto-Downgrade**
- Given: Recognition exceeds 15 seconds
- When: Triggering downgrade
- Then: System reads "Recognition taking too long, auto-switching to backup model", cancels current model, tries next priority model

**AC3: Downgrade Chain**
- Given: Downgrade triggered
- When: Selecting backup model
- Then: Follow priority: UI-TARS GPU → MiniCPM CPU → Doubao API

**AC4: User Cancel**
- Given: Recognition in progress
- When: User presses Esc
- Then: System immediately cancels recognition, reads "Recognition canceled"

**AC5: Timeout Logging**
- Given: Timeout occurs
- When: Logging event
- Then: Record: model name, timeout duration, system resource status (GPU memory, CPU usage)

#### User-Perceivable Changes

**Before Operation**:
- User expects normal wait time

**During Operation**:
- User waits unusually long
- User hears progress updates
- User feels anxious but knows system is working

**After Operation**:
- System auto-downgrades or user cancels
- User regains control
- User can decide whether to wait or give up

#### Constraints (from real.md)

- MUST display progress prompt if inference time > 5 seconds
- MUST auto-downgrade or allow user cancel if > 15 seconds
- Timeout thresholds SHOULD be configurable
- Timeout mechanism MUST prevent threads from hanging indefinitely

#### Dependencies

**Requires**:
- MS-L-01: Trigger First AI Recognition

**Enables**:
- MS-D-05: Handle Cloud API Authorization (downgrade to cloud)

---

### MS-D-05: Handle Cloud API Authorization

**Complex Story**: CS-03
**Story Type**: Dark (Good to Bad to Recovery)
**Priority**: P0
**Story Points**: 5

#### User Story

As a **blind user**,
I want **clear prompts and explicit authorization when system needs to call cloud API (upload screenshots)**,
So that **I clearly understand privacy implications and can choose whether to allow**.

#### Story Evaluation (Dark Story)

**Starting State**: User smoothly uses local models without knowing data processing location

**Turning Point**: Local models all fail (or timeout), system needs to call cloud API

**End State**: System reads "Local models unavailable, use cloud API? This will upload current screenshot to Doubao service, data won't be saved. Y=allow, N=cancel", user makes informed choice

**Emotional Experience**: From unaware → alerted → informed choice → feeling respected

#### Acceptance Criteria

**AC1: First-Time Authorization**
- Given: User first triggers cloud API
- When: Before uploading data
- Then: System reads "About to use cloud API: This will upload current screenshot to Doubao visual recognition service. Data is only used for recognition, won't be saved to server. Do you authorize? Y=allow this time, A=always allow, N=never allow"

**AC2: Authorization Persistence**
- Given: User chooses "Always allow"
- When: Saving config
- Then: Config file sets `enable_cloud_api: true`, subsequent uses don't prompt

**AC3: Privacy Policy Clarity**
- Given: Authorization prompt appears
- When: User presses H (Help)
- Then: System reads detailed privacy policy: data transmission method, service provider, data retention duration

**AC4: Rejection Handling**
- Given: User chooses "Never allow"
- When: Saving config
- Then: Config file sets `enable_cloud_api: false`, system never calls cloud API, downgrade fails reads "Cloud API disabled, recognition failed"

**AC5: Cloud Call Logging**
- Given: Cloud API called
- When: Logging event
- Then: Record: timestamp, screenshot hash (NOT screenshot content), API response time, used model

**AC6: Authorization Revocation**
- Given: User previously chose "Always allow"
- When: User enters settings (NVDA+Shift+C)
- Then: Can modify `enable_cloud_api` setting, choose disable

#### User-Perceivable Changes

**Before Operation**:
- User unaware data might be uploaded to cloud

**During Operation**:
- User hears explicit authorization prompt
- User understands exactly what data will be uploaded
- User given clear choices

**After Operation**:
- If authorized: cloud API called, possibly better results
- If rejected: recognition fails, but user privacy protected
- User feels respected and in control

#### Constraints (from real.md)

- Screenshots MUST prioritize local processing, only call cloud when local fails, with clear user notification
- Cloud API authorization MUST be explicit, NOT implicit defaults
- Config file MUST provide `enable_cloud_api: false` option
- Logs MUST record all cloud API calls for user audit
- Privacy policy MUST be clear and accessible

#### Dependencies

**Requires**:
- MS-D-01: Handle Recognition Failures (or MS-D-04 timeout)
- User configured API key (if needed)

**Enables**:
- MS-L-02: Hear AI Recognition Results (via cloud path)
- MS-L-11: Manage API Keys (advanced users configure own keys)

---

### MS-G-03: Learn to Judge Confidence

**Complex Story**: CS-03
**Story Type**: Grey (Cyclical)
**Priority**: P2
**Story Points**: 1

#### User Story

As a **experienced user**,
I want to **gradually learn which confidence levels are trustworthy through repeated experience**,
So that **I can quickly judge result reliability without viewing details every time**.

#### Story Evaluation (Grey Story)

**Cycle Pattern**: User encounters hundreds of recognition results with varying confidences

**Optimization Point**: Help users build confidence judgment intuition through experience accumulation

**Habit Formation**: From "every uncertain result needs detail check" → "80%+ confidently use, 60-70% cautiously verify" → "confidence calibrated"

**Emotional Experience**: From "need guidance" → "starting to understand patterns" → "confident judgment"

#### Acceptance Criteria

**AC1: Confidence Grading Reading**
- Given: Element confidence = 0.95
- When: Reading element
- Then: Use tone like "very certain" or simply omit confidence (implying high confidence)

**AC2: Medium Confidence Hint**
- Given: Element confidence = 0.72
- When: Reading element
- Then: Read "[element info], confidence acceptable"

**AC3: Low Confidence Warning**
- Given: Element confidence < 0.7
- When: Reading element
- Then: Read "Uncertain: [element info], X% confidence"

**AC4: Historical Accuracy Feedback**
- Given: User uses for 1 month
- When: User triggers statistics (NVDA+Shift+S)
- Then: System reads "Your usage history: 85% confidence results, actual accuracy 92%; 65% confidence results, actual accuracy 58%"

#### User-Perceivable Changes

**Before Operation** (First Month):
- User uncertain about confidence meaning
- May overly rely on or distrust system

**During Operation** (Months 2-3):
- User accumulates experience
- Starts intuitively judging reliability

**After Operation** (After 3 Months):
- User establishes confidence calibration
- Can quickly decide whether to trust or verify

#### Constraints (from real.md)

- Confidence grading SHOULD help users build intuition
- SHOULD provide historical accuracy feedback (if feasible)

#### Dependencies

**Requires**:
- MS-L-08: Understand AI Limitations
- MS-D-02: Handle Low Confidence Results

**Enables**:
- MS-L-10: Configure Confidence Threshold (users know how to set thresholds)

---

## CS-04: Configuration & Optimization

**Description**: Advanced users optimize system behavior through configuration, improving efficiency and experience

**User Type**: Users who have used product for 1+ months, familiar with basic features, want personalized optimization

**Core Value**: Provide customization options, make system adapt to different usage habits and hardware environments

**Story Type Distribution**:
- Light Stories: 2 (master configuration, customize shortcuts)
- Dark Stories: 1 (configuration errors)
- Grey Stories: 1 (configuration adjustment becomes routine)

**User Journey**:
[Discover configuration option] → [Enter settings] → [Adjust parameters] → [Experience improvements] → [Form personal configuration]

---

### MS-L-09: Master Configuration System

**Complex Story**: CS-04
**Story Type**: Light (Bad to Good)
**Priority**: P1
**Story Points**: 3

#### User Story

As a **advanced user**,
I want to **access complete configuration interface via NVDA+Shift+C**,
So that **I can adjust model priority, confidence thresholds, cache settings, etc. to fit my needs**.

#### Story Evaluation (Light Story)

**Starting State**: User uses default config, but wants customization (e.g., GPU priority or CPU priority)

**Turning Point**: User presses NVDA+Shift+C, hears "Settings menu: 1. Model priority, 2. Confidence threshold, 3. Cache settings, 4. Shortcut customization, 5. Privacy settings"

**End State**: User adjusts settings, saves config, system behavior matches expectations

**Emotional Experience**: From "restricted by defaults" → "can customize" → "system listens to me"

#### Acceptance Criteria

**AC1: Settings Access**
- Given: User presses NVDA+Shift+C
- When: Opening settings menu
- Then: System reads "Settings menu" and lists all categories

**AC2: Keyboard Navigation**
- Given: In settings menu
- When: User presses number keys (1-5)
- Then: Enter corresponding settings category

**AC3: Model Priority Configuration**
- Given: In "Model priority" settings
- When: User chooses options
- Then: Can set: "GPU priority", "CPU priority", "Cloud priority", "Disable cloud"

**AC4: Confidence Threshold Configuration**
- Given: In "Confidence threshold" settings
- When: User adjusts value
- Then: Can set 0.5-0.9, default 0.7, system reads "Current threshold 0.7, means confidence <70% marked 'Uncertain', adjust? Input new value or press Enter to keep"

**AC5: Cache Settings**
- Given: In "Cache settings"
- When: User adjusts parameters
- Then: Can set: cache TTL (1-10 minutes), max cache entries (50-200)

**AC6: Config Persistence**
- Given: User modifies settings
- When: Pressing S (Save)
- Then: Write to `config.yaml`, reads "Settings saved, will take effect on next recognition"

**AC7: Reset Defaults**
- Given: In settings menu
- When: User presses R (Reset)
- Then: Restore all defaults, read "Reset to defaults, press S to save"

#### User-Perceivable Changes

**Before Operation**:
- User restricted by defaults
- May feel system doesn't fit their needs

**During Operation**:
- User navigates settings menu
- Adjusts parameters
- Sees immediate feedback

**After Operation**:
- User feels system is "customized"
- System behavior matches expectations
- User satisfaction increases

#### Constraints (from real.md)

- Configuration interface MUST comply with WCAG 2.1 AA, fully keyboard accessible
- All settings MUST have clear descriptions
- Config file SHOULD use YAML format, human-readable
- Settings MUST NOT include sensitive info in plaintext (e.g., API keys encrypted)

#### Dependencies

**Requires**:
- User has used system for some time, understands features

**Enables**:
- MS-L-10: Configure Confidence Threshold
- MS-L-11: Manage API Keys
- MS-L-12: Customize Shortcuts

---

### MS-L-10: Configure Confidence Threshold

**Complex Story**: CS-04
**Story Type**: Light (Bad to Good)
**Priority**: P2
**Story Points**: 2

#### User Story

As a **cautious user**,
I want to **adjust "Uncertain" marking threshold (default 0.7 → 0.8)**,
So that **I see warnings more frequently, reducing misclick risk**.

#### Story Evaluation (Light Story)

**Starting State**: User feels default 0.7 threshold too loose, wants more warnings

**Turning Point**: User enters settings, adjusts threshold to 0.8

**End State**: System now marks 70-80% confidence results as "Uncertain", user feels more secure

**Emotional Experience**: From "worried about misclicks" → "can adjust threshold" → "system protects me"

#### Acceptance Criteria

**AC1: Threshold Adjustable**
- Given: In "Confidence threshold" settings
- When: User inputs new value (0.5-0.9)
- Then: System validates and saves

**AC2: Real-time Effect**
- Given: Threshold adjusted to 0.8
- When: Next recognition
- Then: 70-80% confidence results marked "Uncertain"

**AC3: Threshold Description**
- Given: Adjusting threshold
- When: System provides guidance
- Then: Read "Higher threshold = more warnings but safer; Lower threshold = fewer interruptions but higher risk"

#### User-Perceivable Changes

**Before Operation**:
- User worries about default settings not safe enough

**During Operation**:
- User adjusts threshold
- Immediately understands setting meaning

**After Operation**:
- User sees more "Uncertain" warnings
- User feels more secure
- User learns to balance safety and efficiency

#### Constraints (from real.md)

- Confidence threshold MUST be user-configurable
- Threshold range SHOULD be reasonable (0.5-0.9)

#### Dependencies

**Requires**:
- MS-L-09: Master Configuration System
- MS-G-03: Learn to Judge Confidence

**Enables**:
- MS-D-02: Handle Low Confidence Results (threshold determines warning frequency)

---

### MS-L-11: Manage API Keys

**Complex Story**: CS-04
**Story Type**: Light (Bad to Good)
**Priority**: P1
**Story Points**: 3

#### User Story

As a **user wanting to use cloud API**,
I want to **securely import and manage my Doubao API key in settings**,
So that **I can use cloud recognition when needed without worrying about key leaks**.

#### Story Evaluation (Light Story)

**Starting State**: User has Doubao API key but doesn't know how to configure

**Turning Point**: User enters "Privacy settings" → "Manage API keys", inputs key

**End State**: System encrypts and saves key, reads "API key saved (encrypted), cloud API now available"

**Emotional Experience**: From "have key but can't use" → "system guides me to configure" → "trust system security"

#### Acceptance Criteria

**AC1: Key Import**
- Given: User in "Manage API keys" settings
- When: System prompts "Input Doubao API key"
- Then: User inputs key string, system reads "Received [key length] characters"

**AC2: Key Encryption**
- Given: User completes key input, presses Enter
- When: System saves key
- Then: Use Windows DPAPI or AES-256 encryption, NEVER save as plaintext

**AC3: Key Verification**
- Given: Key saved
- When: System validates key
- Then: Try calling API once, if successful reads "API key validated", if failed reads "Key invalid, please check"

**AC4: Key Viewing Restrictions**
- Given: Key already saved
- When: User enters "Manage API keys"
- Then: Only display "API key configured, press R to replace, D to delete", NOT plaintext

**AC5: Key Deletion**
- Given: User presses D (Delete)
- When: Confirming deletion
- Then: Delete encrypted key, read "API key deleted, cloud API now unavailable"

#### User-Perceivable Changes

**Before Operation**:
- User can't use cloud API
- User worries about key security

**During Operation**:
- User inputs key via secure interface
- System immediately validates

**After Operation**:
- User can use cloud API
- User trusts key is securely stored
- User can delete or replace key anytime

#### Constraints (from real.md)

- User API keys MUST be encrypted in local config, NEVER plaintext or uploaded
- SHOULD use Windows DPAPI or AES-256 encryption
- MUST NEVER write keys to logs or error reports
- Key import/export also MUST be encrypted

#### Dependencies

**Requires**:
- MS-L-09: Master Configuration System

**Enables**:
- MS-D-05: Handle Cloud API Authorization (key prerequisite)

---

### MS-L-12: Customize Shortcuts

**Complex Story**: CS-04
**Story Type**: Light (Bad to Good)
**Priority**: P2
**Story Points**: 2

#### User Story

As a **user with personal habits**,
I want to **modify default shortcut bindings (e.g., change NVDA+Shift+V to NVDA+Alt+V)**,
So that **shortcuts match my muscle memory or avoid conflicts**.

#### Story Evaluation (Light Story)

**Starting State**: User feels default shortcuts conflict with other plugins or aren't ergonomic

**Turning Point**: User enters "Shortcut customization", modifies specific shortcut

**End State**: System saves new binding, reads "Shortcut updated, NVDA+Alt+V now triggers recognition, takes effect after NVDA restart"

**Emotional Experience**: From "shortcuts uncomfortable" → "can customize" → "system adapts to me"

#### Acceptance Criteria

**AC1: Shortcut List**
- Given: In "Shortcut customization" settings
- When: Entering menu
- Then: Read all customizable shortcuts and current bindings

**AC2: Modify Shortcut**
- Given: User selects "Trigger recognition" shortcut
- When: System prompts "Press new shortcut combination"
- Then: User presses new keys, system captures

**AC3: Conflict Detection**
- Given: User sets new shortcut NVDA+T
- When: System validates
- Then: Detect conflicts with NVDA core or other plugins, read "This shortcut conflicts with 'Read window title', choose another"

**AC4: Reset Shortcut**
- Given: In shortcut list
- When: User presses R (Reset)
- Then: Restore specific shortcut to default

#### User-Perceivable Changes

**Before Operation**:
- User feels shortcuts uncomfortable or conflict

**During Operation**:
- User modifies shortcuts
- System immediately validates

**After Operation**:
- User uses comfortable shortcuts
- Muscle memory no longer conflicts
- User satisfaction increases

#### Constraints (from real.md)

- MUST comply with WCAG 2.1 AA, all features keyboard accessible
- Shortcut design SHOULD consider ergonomics
- MUST auto-detect conflicts, warn user

#### Dependencies

**Requires**:
- MS-L-09: Master Configuration System

**Enables**:
- MS-G-01: Repeated Trigger Recognition (customized shortcuts improve efficiency)

---

### MS-D-06: Handle Configuration Errors

**Complex Story**: CS-04
**Story Type**: Dark (Good to Bad to Recovery)
**Priority**: P1
**Story Points**: 2

#### User Story

As a **user who may misconfigure**,
I want **system to validate settings and provide error messages + recovery suggestions**,
So that **incorrect configs don't cause system crashes**.

#### Story Evaluation (Dark Story)

**Starting State**: User smoothly modifies settings

**Turning Point**: User inputs invalid value (e.g., threshold 1.5 > 1.0)

**End State**: System reads "Invalid value: threshold must be 0.5-0.9, current input 1.5, please re-enter"

**Emotional Experience**: From careless → blocked → helped/safe

#### Acceptance Criteria

**AC1: Real-time Validation**
- Given: User inputs setting value
- When: System validates
- Then: Check range, type, format, immediately prompt if invalid

**AC2: Error Message Clarity**
- Given: User inputs invalid value
- When: Generating error message
- Then: Clearly state: 1. What's wrong 2. Valid range 3. How to fix

**AC3: Invalid Config Handling**
- Given: Config file manually edited with invalid values
- When: Plugin loads
- Then: Detect errors, read "Config file error, resetting to defaults", write error log

**AC4: Backup Mechanism**
- Given: User saves new config
- When: System writes file
- Then: First backup old config to `config.yaml.bak`

#### User-Perceivable Changes

**Before Operation**:
- User careless, inputs invalid value

**During Operation**:
- System immediately blocks
- User hears clear error message

**After Operation**:
- User understands what's wrong
- User re-inputs correct value
- System doesn't crash due to invalid config

#### Constraints (from real.md)

- Config errors MUST NOT crash plugin or NVDA
- Error messages MUST be user-friendly
- SHOULD provide config backup/restore mechanism

#### Dependencies

**Requires**:
- MS-L-09: Master Configuration System

**Enables**:
- All configuration stories (provides safety net)

---

### MS-G-04: Configuration Adjustment Becomes Routine

**Complex Story**: CS-04
**Story Type**: Grey (Cyclical)
**Priority**: P2
**Story Points**: 1

#### User Story

As a **experienced user**,
I want to **periodically adjust configs based on changing needs (e.g., switch to CPU priority when working on battery)**,
So that **system always performs optimally in my current environment**.

#### Story Evaluation (Grey Story)

**Cycle Pattern**: User adjusts configs weekly/monthly based on usage scenarios (office vs. mobile, high accuracy vs. speed)

**Optimization Point**: Make config access quick, adjustments take effect immediately

**Habit Formation**: From "stick to defaults" → "actively adjust based on scenarios" → "config tuning becomes routine"

**Emotional Experience**: From "passive acceptance" → "active optimization" → "system mastery"

#### Acceptance Criteria

**AC1: Quick Access**
- Given: User needs to adjust configs
- When: Pressing NVDA+Shift+C
- Then: Immediately open settings menu

**AC2: Preset Profiles**
- Given: User has multiple common scenarios
- When: In settings menu
- Then: Can save/load preset profiles, e.g., "Office mode (GPU priority)", "Mobile mode (CPU priority)"

**AC3: Profile Switching**
- Given: User switches work scenarios
- When: Pressing NVDA+Shift+C → P (Profiles)
- Then: Quickly switch preset profiles, e.g., "Switched to Mobile mode: CPU priority, cache TTL extended to 10 minutes"

#### User-Perceivable Changes

**Before Operation** (First Months):
- User rarely adjusts settings
- May not realize optimization potential

**During Operation** (Months 3-6):
- User discovers config tuning fun
- Starts actively experimenting

**After Operation** (After 6 Months):
- User has stable config habits
- Config tuning becomes routine maintenance
- User feels complete system mastery

#### Constraints (from real.md)

- Config system SHOULD support preset profiles
- Profile switching SHOULD be quick (< 3 steps)

#### Dependencies

**Requires**:
- MS-L-09: Master Configuration System
- MS-L-10: Configure Confidence Threshold
- MS-L-08: Configure Model Priority

**Enables**:
- Users optimize system based on scenarios, maximize value

---

# Story Map

## User Journey Overview

```
Phase 1: Onboarding          Phase 2: Core Usage         Phase 3: Trust Building    Phase 4: Power User
(First Day)                  (Week 1-4)                  (Month 2-3)                (Month 4+)
    |                            |                            |                          |
    v                            v                            v                          v
[First Trigger]──────────>[Navigate & Activate]───────>[Handle Uncertainty]───────>[Optimize Config]
    │                            │                            │                          │
    ├─ MS-L-01                  ├─ MS-L-04                  ├─ MS-L-08                ├─ MS-L-09
    ├─ MS-L-02                  ├─ MS-L-05                  ├─ MS-D-04                ├─ MS-L-10
    ├─ MS-L-03                  ├─ MS-L-06                  ├─ MS-D-05                ├─ MS-L-11
    ├─ MS-D-01                  ├─ MS-L-07                  ├─ MS-G-03                ├─ MS-L-12
    └─ MS-G-01                  ├─ MS-D-02                                            ├─ MS-D-06
                                ├─ MS-D-03                                            └─ MS-G-04
                                └─ MS-G-02

Emotional Arc:
Confusion → Hope → Control → Trust → Mastery
```

## Iteration Planning

### Iteration 1: MVP (2 weeks)

**Goal**: Enable users to complete basic flow "trigger recognition → hear results → navigate → activate"

**Stories**:
- MS-L-01: Trigger First AI Recognition (P0, 3 points)
- MS-L-02: Hear AI Recognition Results (P0, 5 points)
- MS-L-04: Navigate to Specific Element (P0, 3 points)
- MS-L-05: Activate Target Element (P0, 5 points)
- MS-D-01: Handle Recognition Failures (P0, 3 points)
- MS-D-02: Handle Low Confidence Results (P0, 3 points)

**Story Type Distribution**:
- Light: 4 stories (66%) - Main value delivery
- Dark: 2 stories (34%) - Essential error handling

**Expected User Value**:
- Users can use AI recognition to independently operate Feishu/DingTalk
- Users understand system capabilities and limitations
- Users experience affordance recovery (from "can't see" to "can operate")

---

### Iteration 2: Reliability & Efficiency (2 weeks)

**Goal**: Enhance system reliability and daily usage efficiency

**Stories**:
- MS-L-03: Experience Cache Acceleration (P1, 2 points)
- MS-L-06: Manual Re-recognize (P1, 2 points)
- MS-L-07: View Element Details (P2, 2 points)
- MS-D-03: Handle Click Failures (P1, 2 points)
- MS-D-04: Handle Recognition Timeout (P0, 3 points)
- MS-D-05: Handle Cloud API Authorization (P0, 5 points)
- MS-G-01: Repeated Trigger Recognition (P1, 1 point)
- MS-G-02: Daily Element Navigation (P1, 1 point)

**Story Type Distribution**:
- Light: 3 stories (37%) - Experience optimization
- Dark: 3 stories (37%) - Complete error handling
- Grey: 2 stories (26%) - Habit formation

**Expected User Value**:
- Recognition speed significantly improves (cache hit)
- Exception handling complete, users feel secure
- Daily operations become smooth habits

---

### Iteration 3: Trust & Transparency (2 weeks)

**Goal**: Build user trust through transparency, cultivate rational AI literacy

**Stories**:
- MS-L-08: Understand AI Limitations (P1, 2 points)
- MS-G-03: Learn to Judge Confidence (P2, 1 point)

**Story Type Distribution**:
- Light: 1 story (50%) - Cognitive growth
- Grey: 1 story (50%) - Experience accumulation

**Expected User Value**:
- Users establish realistic AI expectations
- Users learn to judge result reliability
- User-system trust deepens

---

### Iteration 4: Customization & Mastery (2 weeks)

**Goal**: Provide advanced customization, enable power users to maximize system value

**Stories**:
- MS-L-09: Master Configuration System (P1, 3 points)
- MS-L-10: Configure Confidence Threshold (P2, 2 points)
- MS-L-11: Manage API Keys (P1, 3 points)
- MS-L-12: Customize Shortcuts (P2, 2 points)
- MS-D-06: Handle Configuration Errors (P1, 2 points)
- MS-G-04: Configuration Adjustment Becomes Routine (P2, 1 point)

**Story Type Distribution**:
- Light: 4 stories (67%) - Empowerment
- Dark: 1 story (17%) - Safety net
- Grey: 1 story (17%) - Advanced habits

**Expected User Value**:
- Users can personalize system
- System adapts to different usage scenarios
- Users feel complete system mastery

---

# Implementation Recommendations

## Story Type Distribution Analysis

### Overall Distribution (19 stories total)

| Story Type | Count | Percentage | Target (MVP) | Deviation |
|-----------|-------|-----------|--------------|-----------|
| Light | 9 | 47% | 70% | -23% |
| Dark | 6 | 32% | 20% | +12% |
| Grey | 4 | 21% | 10% | +11% |

**Analysis**:
- Light story proportion lower than ideal: Due to MVP requiring comprehensive error handling (real.md constraints)
- Dark story proportion higher: Privacy, security, stability constraints require robust exception handling
- Grey story proportion higher: Considering long-term daily usage experience
- **Recommendation**: Merge some Dark stories if feasible, or add more Light stories (e.g., first successful operation tutorials, achievement system)

### Iteration Distribution

| Iteration | Light | Dark | Grey | Characteristics |
|----------|-------|------|------|----------------|
| Iter 1 (MVP) | 4 (66%) | 2 (34%) | 0 (0%) | Quick value delivery, essential error handling |
| Iter 2 | 3 (37%) | 3 (37%) | 2 (26%) | Balance reliability and efficiency |
| Iter 3 | 1 (50%) | 0 (0%) | 1 (50%) | Focus on trust building |
| Iter 4 | 4 (67%) | 1 (17%) | 1 (17%) | Empower power users |

---

## Risk Assessment

### Technical Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|------------|------------|
| Model loading failure | High | Medium | MS-D-01 handles, provides Cloud API fallback |
| GPU memory insufficient | High | Medium | Auto-detect and downgrade to CPU/Cloud |
| NVDA plugin conflicts | Medium | Low | Extensive testing, shortcut conflict detection |
| Cache management complexity | Low | Medium | Use proven cache libraries, comprehensive testing |
| API key encryption failure | High | Low | Use Windows DPAPI, fallback to AES-256 |

### User Experience Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|------------|------------|
| Inference too slow | High | High | Progress prompts + auto-downgrade (MS-D-04) |
| Low recognition accuracy | High | Medium | Confidence transparency + re-recognize mechanism |
| User doesn't understand AI limitations | Medium | High | MS-L-08 gradual education |
| Shortcut conflicts | Medium | Medium | MS-L-12 customization + conflict detection |
| Privacy concerns | High | Low | MS-D-05 explicit authorization + transparency |

---

## Dependencies & Prerequisites

### External Dependencies

- **NVDA Core**: Version 2023.1 - 2024.4
- **Python**: 3.11 (NVDA recommended)
- **UI-TARS Model**: 7B version (or quantized version)
- **MiniCPM-V Model**: 2.6 version
- **Doubao API**: Valid API key (optional)
- **Hardware**:
  - GPU: NVIDIA, CUDA 11.8+, VRAM ≥8GB (recommended ≥16GB)
  - CPU: x64, AVX2 support
  - Memory: ≥8GB (recommended 16GB)
  - Storage: ≥50GB (for models)

### Internal Dependencies (Between Stories)

Critical Path:
```
MS-L-01 (Trigger)
    ↓
MS-L-02 (Results)
    ↓
MS-L-04 (Navigate)
    ↓
MS-L-05 (Activate)
```

Enhancement Paths:
```
MS-L-02 → MS-L-03 (Cache)
MS-L-05 → MS-D-02 (Low Confidence) → MS-L-06 (Re-recognize)
MS-D-01 → MS-D-04 (Timeout) → MS-D-05 (Cloud Auth) → MS-L-11 (API Keys)
```

---

# Testing Guide

## Test Categories

### 1. Light Story Tests (User Growth)

**Goal**: Verify users can successfully complete growth journey from "can't do" to "can do"

**Test Scenarios**:

**MS-L-01: First Trigger**
- [ ] User presses NVDA+Shift+V, immediately hears "Recognizing screen..."
- [ ] Plugin doesn't affect other NVDA operations during recognition
- [ ] First use prompts "First use? Data processed locally"

**MS-L-02: Hear Results**
- [ ] Recognition completes within 15 seconds
- [ ] Results format: "Found X buttons: [list], Y input fields: [list]"
- [ ] Low-confidence elements marked "Uncertain"
- [ ] Empty results prompt "No interactive elements found, suggestions..."

**MS-L-03: Cache Acceleration**
- [ ] Second recognition of same window returns < 100ms
- [ ] Cache details display "This result from cache, recognized X seconds ago"
- [ ] Cache auto-expires after 5 minutes

---

### 2. Dark Story Tests (Problem Handling)

**Goal**: Verify system properly handles exceptions, provides recovery paths

**Test Scenarios**:

**MS-D-01: Recognition Failure**
- [ ] GPU OOM triggers auto-downgrade to CPU model
- [ ] Error message clear: "Recognition failed: [reason], [suggestion]"
- [ ] All models fail, final prompt provides 3 recovery suggestions
- [ ] Plugin crash doesn't affect NVDA main program

**MS-D-02: Low Confidence**
- [ ] < 0.7 confidence elements automatically prefixed "Uncertain"
- [ ] Activation prompts "This element uncertain (X% confidence), confirm? Y/N/R"
- [ ] Press N cancels operation, press R re-recognizes

**MS-D-04: Timeout**
- [ ] Recognition > 5 seconds displays progress "Recognizing, X seconds elapsed..."
- [ ] Recognition > 15 seconds auto-downgrades or prompts cancel
- [ ] User presses Esc immediately cancels recognition

**MS-D-05: Cloud Authorization**
- [ ] First cloud call prompts "Will upload screenshot, authorize? Y/A/N"
- [ ] Choose "Always allow" saves to config, subsequent uses don't prompt
- [ ] Choose "Never allow", system never calls cloud API
- [ ] API calls logged (timestamp, hash, no screenshot content)

---

### 3. Grey Story Tests (Daily Operations)

**Goal**: Verify repeated operations smooth, support habit formation

**Test Scenarios**:

**MS-G-01: Repeated Trigger**
- [ ] NVDA+Shift+V shortcut consistent, doesn't conflict
- [ ] Shortcut ergonomic, easily pressed
- [ ] High-frequency usage (50+ times/day), ≥30% cache hit

**MS-G-02: Daily Navigation**
- [ ] N/P keys immediately respond (< 100ms)
- [ ] Continuous navigation simplified format, only reads type and text
- [ ] List end returns to start with hint

**MS-G-03: Confidence Judgment**
- [ ] High confidence (>80%) omits confidence reading
- [ ] Medium confidence (70-80%) reads "confidence acceptable"
- [ ] Low confidence (<70%) reads "Uncertain: X%"

---

## Acceptance Test Checklist

### Phase 1: Core Functionality (Iter 1)

- [ ] User can trigger recognition and hear results
- [ ] User can navigate elements and activate targets
- [ ] System handles recognition failures without crashing
- [ ] Low-confidence elements have warnings
- [ ] Complete flow from triggering to activating < 30 seconds

### Phase 2: Reliability (Iter 2)

- [ ] Cache hit rate ≥30% (repeated recognition scenarios)
- [ ] Timeout auto-downgrade works properly
- [ ] Cloud API authorization flow correct
- [ ] Click failures have clear error messages
- [ ] All exceptions don't affect NVDA stability

### Phase 3: Trust (Iter 3)

- [ ] Users understand AI isn't 100% accurate
- [ ] Users learn to judge confidence reliability
- [ ] System transparency builds user trust

### Phase 4: Customization (Iter 4)

- [ ] Users can access and modify all config items
- [ ] API keys encrypted, not viewable in plaintext
- [ ] Shortcut customization works, conflict detection effective
- [ ] Invalid configs have clear error prompts, don't crash system

---

## Constraint Compliance Testing (from real.md)

### Privacy & Security

- [ ] Screenshots prioritize local processing
- [ ] Cloud API calls require explicit user authorization
- [ ] API keys encrypted with Windows DPAPI or AES-256
- [ ] Logs don't include sensitive info (screenshots, keys)

### User Experience

- [ ] All recognition results include confidence scores
- [ ] Low-confidence results (< threshold) marked "Uncertain"
- [ ] Inference > 5 seconds displays progress prompts
- [ ] Inference > 15 seconds auto-downgrades or allows cancel
- [ ] All functions have keyboard shortcuts
- [ ] All UI elements have clear NVDA reading labels

### System Stability

- [ ] Time-consuming operations in independent threads
- [ ] Comprehensive exception handling (try-except)
- [ ] Plugin crashes don't affect NVDA main program
- [ ] Timeout mechanisms prevent thread hangs

### Compliance

- [ ] Complies with WCAG 2.1 AA accessibility standards
- [ ] Open source license Apache 2.0 or MIT
- [ ] Dependency library licenses compatible
- [ ] Commercial use permissions clearly stated

---

# Appendix

## Story Size Guidelines

| Story Points | Time | Complexity | Examples |
|-------------|------|-----------|----------|
| 1 | Half day | Very simple | Grey stories, config toggles |
| 2 | 1 day | Simple | Cache implementation, shortcuts |
| 3 | 1-2 days | Medium | Recognition trigger, navigation |
| 5 | 2-3 days | Complex | Result processing, element activation |
| 8 | 3-5 days | Very complex | Multi-model downgrade, cloud auth |

## INVEST Principles Verification

All minimal stories follow INVEST principles:

- **I**ndependent: Each story can be developed/tested independently
- **N**egotiable: Details discussed and adjusted during development
- **V**aluable: Each story brings clear user value
- **E**stimable: Work effort can be estimated (1-5 story points)
- **S**mall: Can be completed within 1-3 days
- **T**estable: Clear acceptance criteria

## Story Type Distribution Recommendations

### MVP Stage (First 3 Months)

- **Light Stories**: 60-70% (Quick value demonstration)
- **Dark Stories**: 20-30% (Essential error handling)
- **Grey Stories**: 5-15% (Laying efficiency foundation)

### Growth Stage (Months 4-12)

- **Light Stories**: 40-50% (Continue adding value)
- **Dark Stories**: 30-40% (Comprehensive exception handling)
- **Grey Stories**: 20-30% (Optimize daily experience)

### Mature Stage (After 1 Year)

- **Light Stories**: 30-40% (Incremental innovation)
- **Dark Stories**: 20-30% (Maintain robustness)
- **Grey Stories**: 40-50% (Deep habit optimization)

---

## Document Metadata

**Total Stories**: 19 (9 Light, 6 Dark, 4 Grey)
**Total Story Points**: 51 points
**Estimated Development Time**: 8 weeks (4 iterations × 2 weeks)
**Target Users**: Visually impaired NVDA users (200+ million global blind population)
**Core Value Proposition**: Use AI to restore affordances, enable independent use of custom UI apps

**Generated with**: pm-user-story skill (Three Minimal Stories Framework)
**Quality Assurance**:
- [x] All core features have corresponding minimal stories
- [x] Each minimal story follows INVEST principles
- [x] Story type distribution reasonable (emphasis on Light, supplemented by Dark and Grey)
- [x] Acceptance criteria clear and testable
- [x] Dependencies identified
- [x] Story size appropriate (1-5 story points)
- [x] Reflects real.md constraints
- [x] Story map shows clear user journey
- [x] Each story has clear user value

---

**Document End**

**Next Steps**:
1. Development team reviews and refines story details
2. UI design team creates interfaces based on stories (especially settings)
3. QA team develops test cases based on acceptance criteria
4. Product owner prioritizes backlog, determines first iteration stories
5. Development begins, progress tracked via story map
